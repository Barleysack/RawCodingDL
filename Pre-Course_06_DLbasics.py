import numpy as np
#softmax: 이 함수를 통해 특정 벡터가 어떤 클래스에 속할 확률이 있는지 알 수 있다. 

def softmax(vec): 
    denum = np.exp(vec - np.max(vec,axis=-1,keepdims=True))
    num = np.sum(denum, axis = -1 , keepdims=True)
    val = denum/num
    return val
#추론을 할때 원-핫 벡터로 최대값을 가진 주소만 1로 출력하는 연산을 사용해 softmax를 사용하지 않는다. 

#활성함수: 해당 주소의 출력값을 사용한다. 실수값을 받고, 뱉는다... 
#선형모델 출력물을 비선형으로 변환하고, 이를 잠재벡터라고 부른
# 
# 다. 이로서 만들어 지는 것이 바로 신경망. 
# 활성함수는 실수공간에 정의된 비선형 함수로, 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없다. 
# sigmoid 함수나 tanh 함수는 전통적으로 많이 쓰였으나 딥러닝에서는 ReLU를 많이 쓴다. 
# 잠재백터 h에서 가중치행력 w와 b를 통해 다시한번 활성화함수를 통한 선형변환해 출력하면 두 식의 가중치등을 패러미터로 가진 2층 신경망이 된다. 
#층이 깊을수록 목적함수를 근사하는데 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능하다. 
#층이 얇으면 필요한 뉴런 갯수가 기하급수적으로 늘어나기에 넓은 신경망이 되어야 한다 .






#역전파는 컴퓨터가 각 노드의 텐서값을 기억해야한다. 
