import numpy as np
import math
import sympy as sym

#np.linalg.pinv를 이용하면 데이터를
#선형모델로 해석하는 선형 회귀식을 찾을 수 있다.
#무어-펜로즈 안쓰고 경사하강법을 하는걸 알아보자. 
#선형회귀 목적식은 abs(y-Xb)이고 이를 최소화하는 b를 찾는 것.
#l2노름보다 l2노름의 제곱을 사용하면 식이 조금 더 간단해진다. 
#어느쪽이든 최소화하는 B를 찾는건 같기 때문...

"""Pseudo Code for GD/Linear Regression

Input: X,y,LR,T
Output:beta
#norm:L2-노름을 계산하는 함수
#LR : 학습률
#T: 학습횟수

for t in range(T):
    error = y-X (베타)
    grad = -transpose(X)@error
    beta = beta -lr * grad
    """

#허나 함수의 모양에 따라 경사하강법은 항상 수렴하지는 않는다.
#이를 위하여 SGD를 써본다. 볼록이 아닌 목적식을 최적화하는데 사용한다.
#데이터를 일부를 미니배치로서 사용하는 SGD. -미니배치-SGD
#실증적으로 낫다. 또한 데이터의 일부만을 사용하기에 연산 자원을 효율적으로 사용할 수 있다.
#매번 미니배치로 그래디언트 벡터를 계산하므로 지금의 목적식이 아닌 좁은 구간의 근사한 목적식을 사용한다.
#미니배치는 확률적으로 선택하므로 목적식 모양이 바뀌게 된다.
#목적식이 바뀐 상태이기에, 극소점에서 탈출하는게 가능하게 된다.
#정확하지 않은 방향이어도 한번에 많이 움직여 빠르다.
#둘 다 학습률, 학습횟수를 잘 고려해야함다...
#sgd에선 미니배치 사이즈 또한 잘 고려해야 한다. 


